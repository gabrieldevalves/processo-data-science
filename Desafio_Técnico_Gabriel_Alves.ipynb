{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Desafio T√©cnico ‚Äì Gabriel Alves\n",
        "\n",
        "##  Descri√ß√£o do Projeto:\n",
        "Este notebook foi desenvolvido como parte do processo seletivo para a vaga de Cientista de Dados, com o objetivo de construir um chatbot de an√°lise de dados com LLM."
      ],
      "metadata": {
        "id": "1N6L0_su6hep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala√ß√£o de bibliotecas\n",
        "\n",
        "!pip install boto3 pandas openai gradio matplotlib --quiet"
      ],
      "metadata": {
        "id": "C8YRZpez6gDh"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa√ß√£o de bibliotecas\n",
        "\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "import sqlite3\n",
        "import gzip\n",
        "from io import BytesIO, StringIO\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "bcUgoUEXYjTJ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Credenciais de acesso e configura√ß√µes\n",
        "\n",
        "aws_access_key = 'Insira sua credencial aqui'\n",
        "aws_secret_key = 'Insira sua credencial aqui'\n",
        "client = OpenAI(api_key='Insira sua credencial aqui')\n",
        "\n",
        "bucket_name = 'bucket-desafio-ds'\n",
        "\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    region_name='us-east-2',\n",
        "    aws_access_key_id=aws_access_key,\n",
        "    aws_secret_access_key=aws_secret_key\n",
        ")"
      ],
      "metadata": {
        "id": "ql65lcVKYyLS"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para validar a exist√™ncia do arquivo no bucket da S3\n",
        "\n",
        "def detect_file():\n",
        "    response = s3.list_objects_v2(Bucket=bucket_name)\n",
        "    arquivos = [obj['Key'] for obj in response.get('Contents', [])]\n",
        "    if 'train.csv.gz' in arquivos:\n",
        "        return 'train.csv.gz', 'GZIP'\n",
        "    elif 'train.csv' in arquivos:\n",
        "        return 'train.csv', 'NONE'\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Arquivo esperado n√£o encontrado no bucket.\")\n"
      ],
      "metadata": {
        "id": "xBkIj-tlY5ya"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fun√ß√£o para gerar a query SQL via GPT\n",
        "\n",
        "def generate_sql_query(consulta):\n",
        "    prompt = f\"\"\"\n",
        "Voc√™ √© um assistente especializado em converter perguntas em queries SQL para arquivos CSV.\n",
        "A base possui as colunas: REF_DATE, TARGET, VAR2, IDADE, VAR4, VAR5, VAR8.\n",
        "Gere uma query SQL para responder √† pergunta: \"{consulta}\"\n",
        "Considere que:\n",
        "- TARGET: 0 √© adimplente, 1 √© inadimplente\n",
        "- VAR2 √© sexo M para masculino ou F para feminino, VAR4 FLAG DE √ìBITO, VAR5 √© UF, VAR8 √© classe social estimada\n",
        "- A tabela se chama \"s3object\"\n",
        "- Use aspas duplas para nomes de colunas.\n",
        "\"\"\"\n",
        "    resposta = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    conteudo = resposta.choices[0].message.content\n",
        "    conteudo = conteudo.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
        "    return conteudo"
      ],
      "metadata": {
        "id": "mRi9Sdlkl7Kc"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fun√ß√£o de leitura dos dados via SQL\n",
        "\n",
        "def local_query(sql_query, object_key, compress_type):\n",
        "    response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
        "    content = response['Body'].read()\n",
        "    if compress_type == 'GZIP':\n",
        "        content = gzip.decompress(content)\n",
        "    csv_str = content.decode('utf-8')\n",
        "    df = pd.read_csv(StringIO(csv_str))\n",
        "\n",
        "    conn = sqlite3.connect(\":memory:\")\n",
        "    df.to_sql(\"s3object\", conn, index=False, if_exists='replace')\n",
        "    resultado_df = pd.read_sql_query(sql_query, conn)\n",
        "    conn.close()\n",
        "    return resultado_df"
      ],
      "metadata": {
        "id": "yfab8s1CmJol"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fun√ß√£o de gera√ß√£o de insights via GPT\n",
        "\n",
        "def generate_insights(df, consulta):\n",
        "\n",
        "    # obs: A quantidade de linhas do recorte do dataframe a serem analisadas para gera√ß√£o de insights pode ser ajustado no par√¢metro 'df.head'\n",
        "\n",
        "    if df.empty:\n",
        "        return \"Consulta n√£o retornou dados para gerar insights.\"\n",
        "\n",
        "    descricao = f\"\"\"\n",
        "Voc√™ √© um analista de dados. Aqui est√£o os dados resultantes da seguinte consulta: \"{consulta}\".\n",
        "Gere 2 a 3 insights autom√°ticos baseados no DataFrame a seguir:\n",
        "\n",
        "{df.head(1000).to_string(index=False)}\n",
        "\n",
        "Seja direto, t√©cnico e objetivo.\n",
        "\"\"\"\n",
        "    resposta = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": descricao}],\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return resposta.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "_19BbwWbuc6D"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para gerar o gr√°fico via matplotlib\n",
        "\n",
        "def generate_plot(df, consulta):\n",
        "\n",
        "    # Valida√ß√£o m√≠nima para gerar gr√°fico\n",
        "    if df.empty:\n",
        "        return None\n",
        "\n",
        "    # Detectar colunas\n",
        "    cols = df.columns.tolist()\n",
        "    if len(cols) < 2:\n",
        "        return None\n",
        "\n",
        "    # Detectar colunas para eixo x e y\n",
        "    # x: primeira coluna n√£o num√©rica ou de data, y: primeira num√©rica\n",
        "    x_col = None\n",
        "    y_col = None\n",
        "    for c in cols:\n",
        "        if pd.api.types.is_numeric_dtype(df[c]):\n",
        "            if y_col is None:\n",
        "                y_col = c\n",
        "        else:\n",
        "            if x_col is None:\n",
        "                x_col = c\n",
        "\n",
        "    if x_col is None or y_col is None:\n",
        "        # fallback: plot s√≥ a primeira coluna num√©rica como histograma\n",
        "        plt.figure(figsize=(8,5))\n",
        "        plt.hist(df[cols[0]])\n",
        "        plt.title(\"Histograma de \" + cols[0])\n",
        "        plt.tight_layout()\n",
        "        return plt\n",
        "\n",
        "    # Gr√°fico de barras agrupando valores √∫nicos do x_col\n",
        "    plt.figure(figsize=(8,5))\n",
        "    try:\n",
        "        grouped = df.groupby(x_col)[y_col].mean().sort_values(ascending=False)\n",
        "        grouped.plot(kind='bar')\n",
        "        plt.title(f\"M√©dia de {y_col} por {x_col}\")\n",
        "        plt.xlabel(x_col)\n",
        "        plt.ylabel(f\"M√©dia de {y_col}\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        return plt\n",
        "    except Exception as e:\n",
        "        return None"
      ],
      "metadata": {
        "id": "gRHRfq880K92"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o de salvar o .csv\n",
        "\n",
        "def save_csv(df):\n",
        "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
        "    df.to_csv(temp_file.name, index=False)\n",
        "    return temp_file.name"
      ],
      "metadata": {
        "id": "NTV7Z3C10OOj"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o principal\n",
        "\n",
        "def execute_pipeline(consulta):\n",
        "    try:\n",
        "        # Detecta arquivo no S3\n",
        "        object_key, compress_type = detect_file()\n",
        "\n",
        "        # Gera SQL com OpenAI\n",
        "        sql_query = generate_sql_query(consulta)\n",
        "\n",
        "        # Consulta e retorna dataframe\n",
        "        df = local_query(sql_query, object_key, compress_type)\n",
        "\n",
        "        if df.empty:\n",
        "            return (\"\", \"Consulta retornou 0 linhas.\", \"Sem dados para insights.\", None, None)\n",
        "\n",
        "        # Forma√ß√£o de datas\n",
        "        if 'REF_DATE' in df.columns:\n",
        "            df['REF_DATE'] = pd.to_datetime(df['REF_DATE'], errors='coerce').dt.strftime('%d/%m/%Y')\n",
        "\n",
        "        # Gerar insights\n",
        "        insights = generate_insights(df, consulta)\n",
        "\n",
        "        # Gerar gr√°fico matplotlib\n",
        "        plt_fig = generate_plot(df, consulta)\n",
        "\n",
        "        # Salvar CSV para download\n",
        "        arquivo_path = save_csv(df)\n",
        "\n",
        "        # Preparar sa√≠da texto para tabela e SQL\n",
        "        tabela_preview = df.head(50).to_string(index=False)\n",
        "\n",
        "        return (sql_query, tabela_preview, insights, plt_fig, arquivo_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        erro_msg = f\"O c√≥digo teve o seguinte erro: {str(e)}\"\n",
        "        return (\"\", erro_msg, \"\", None, None)"
      ],
      "metadata": {
        "id": "9gsVeIib0pBE"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interface Gradio\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"violet\", secondary_hue=\"gray\")) as demo:\n",
        "    gr.Markdown(\"## üìä Desafio Data Science - Gabriel Alves\")\n",
        "    consulta = gr.Textbox(label=\"üîç Insira seu prompt para consulta (ex: 'inadimpl√™ncia por UF')\")\n",
        "    executar = gr.Button(\"Executar\")\n",
        "\n",
        "    sql_output = gr.Textbox(label=\"üß† SQL Gerada\", interactive=False)\n",
        "    tabela = gr.Textbox(label=\"üìÑ Resultado dos Dados\", lines=10, interactive=False)\n",
        "    insights_box = gr.Textbox(label=\"üí° Insights\", lines=6, interactive=False)\n",
        "    grafico_output = gr.Plot(label=\"üìà Gr√°fico Gerado\")\n",
        "    arquivo_csv = gr.File(label=\"üìÅ Baixar CSV\", file_types=[\".csv\"])\n",
        "\n",
        "    executar.click(\n",
        "        fn=execute_pipeline,\n",
        "        inputs=consulta,\n",
        "        outputs=[sql_output, tabela, insights_box, grafico_output, arquivo_csv]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "n1gYeQod0saD",
        "outputId": "a33ce14c-4a1e-4dcb-f674-338e5e7d1210"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ac538abb778bc42c19.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ac538abb778bc42c19.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    }
  ]
}